{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54f6afbd",
   "metadata": {},
   "source": [
    "# Save the county details for every NOAA storm event into a Postgres table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f29a4a",
   "metadata": {},
   "source": [
    "## Install the Python requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e69f271",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%bash\n",
    "!pip install pandas==1.3.2\n",
    "!pip install tqdm==4.62.2\n",
    "!pip install psycopg2-binary==2.9.1\n",
    "!pip install sqlalchemy==1.4.23"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c66702e",
   "metadata": {},
   "source": [
    "## Import the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c12ea77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import io\n",
    "import requests\n",
    "import psycopg2\n",
    "\n",
    "from tqdm.auto import tqdm  # for notebooks\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from amazon_cred import ENDPOINT, PORT, USER, PASSWORD, DATABASE\n",
    "from cred import CENSUS_API_KEY\n",
    "from data_constants import NOAA_CSVFILES_URL, ZONE_COUNTY_CORR_URL, ZONE_COUNTY_CORR_CSV, TABLE_COLUMNS_NOAA\n",
    "from data_constants import MIN_DAMAGE, BEGIN_YEAR, END_YEAR, STORM_CATEGORIES, BLS_CPI_CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49a10db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>period</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>256.974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019</td>\n",
       "      <td>11</td>\n",
       "      <td>257.208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019</td>\n",
       "      <td>10</td>\n",
       "      <td>257.346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019</td>\n",
       "      <td>9</td>\n",
       "      <td>256.759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019</td>\n",
       "      <td>8</td>\n",
       "      <td>256.558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>2020</td>\n",
       "      <td>5</td>\n",
       "      <td>256.394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>256.389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>2020</td>\n",
       "      <td>3</td>\n",
       "      <td>258.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>258.678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "      <td>257.971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>260 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     year  period    value\n",
       "0    2019      12  256.974\n",
       "1    2019      11  257.208\n",
       "2    2019      10  257.346\n",
       "3    2019       9  256.759\n",
       "4    2019       8  256.558\n",
       "..    ...     ...      ...\n",
       "255  2020       5  256.394\n",
       "256  2020       4  256.389\n",
       "257  2020       3  258.115\n",
       "258  2020       2  258.678\n",
       "259  2020       1  257.971\n",
       "\n",
       "[260 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(BLS_CPI_CSV, index_col=False)\n",
    "df = df.drop(columns=['Unnamed: 0'])\n",
    "df.to_csv('bls')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e520f7f4",
   "metadata": {},
   "source": [
    "## Constants\n",
    "\n",
    "- North American Industry Classification System Code\n",
    "- Regex pattern to remove square brackets from the response from census api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfb8235",
   "metadata": {},
   "outputs": [],
   "source": [
    "TABLE_COLUMNS_CENSUS = ['ESTAB', 'PAYANN', 'EMP', 'NESTAB', 'NRCPOPT', 'POP']\n",
    "\n",
    "NAICS_CODE_DICT = { # North American Industry Classification System Code\n",
    "    '11': 'Agriculture, Forestry, Fishing, Hunting',\n",
    "    '21': 'Mining',\n",
    "    '22': 'Utilities',\n",
    "    '23': 'Construction',\n",
    "    '31-33': 'Manufacturing',\n",
    "    '42': 'Wholesale',\n",
    "    '44-45': 'Retail',\n",
    "    '48-49': 'Transportation and Warehousing',\n",
    "    '51': 'Information',\n",
    "    '52': 'Finance and Insurance',\n",
    "    '53': 'Real Estate Rental and Leasing',\n",
    "    '54': 'Professional, Scientific, and Technical Services',\n",
    "    '55': 'Management of Companies and Enterprises',\n",
    "    '56': 'Administrative and Support and Waste Management and Remediation Services',\n",
    "    '61': 'Educational Services',\n",
    "    '62': 'Health Care and Social Assistance',\n",
    "    '71': 'Arts, Entertainment, and Recreation',\n",
    "    '72': 'Accommodation and Food Services',\n",
    "    '81': 'Other Services (except Public Administration)',\n",
    "    '92': 'Public Administration'\n",
    "}\n",
    "\n",
    "PATTERN = '\\[\\[|\\[|\\],|\\]\\]'\n",
    "PATTERN = re.compile(PATTERN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623938a5",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc662104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_csvfiles(url):\n",
    "    html = pd.read_html(url) # read_html and then get the list of files from the html table\n",
    "    df = html[0]\n",
    "    df.drop(columns=['Description'], inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "    df = df[df['Name'].str.contains('StormEvents_details.*d20', regex=True)]\n",
    "\n",
    "    files_dict = {}\n",
    "    for fname in df['Name']:\n",
    "        result = re.findall('_d(?P<year>\\d{4})', fname) # parse the year from the file name\n",
    "        if len(result) > 0:\n",
    "            files_dict[result[0]] = fname # dict key is the year, value is the filename\n",
    "    return files_dict\n",
    "\n",
    "def download_zone_county_corr_file_orig():\n",
    "    df = pd.read_csv(ZONE_COUNTY_CORR_URL, sep='|', header=None, # Pipe delimited text\n",
    "                names=['STATE', 'ZONE', 'CWA', 'NAME', 'STATE_ZONE',\n",
    "                       'CZ_NAME', 'FIPS', 'TIME_ZONE', 'FE_AREA', 'LAT', 'LON'])\n",
    "    df['STATE_FIPS'] = df['FIPS']//1000\n",
    "    df = df[['STATE_FIPS', 'ZONE', 'FIPS']]\n",
    "    df.to_csv('../data/zone_county_corr.csv', index=False)\n",
    "    return df\n",
    "\n",
    "def download_zone_county_corr_file(url):\n",
    "    df = pd.read_csv(url)\n",
    "    return df\n",
    "\n",
    "def get_zone_county_corr_df():\n",
    "    return download_zone_county_corr_file(ZONE_COUNTY_CORR_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f6dd5e",
   "metadata": {},
   "source": [
    "## Retrieve storm data from NOAA\n",
    "\n",
    "Data preparation steps:\n",
    "\n",
    "1. Drop rows that have STATE_FIPS = NaN or CZ_FIPS = NaN\n",
    "2. Convert DAMAGE_PROPERTY, DAMAGE_CROPS to numbers (replace Nan with 0)\n",
    "3. Select only counties in 50 US states and Washington DC (FIPS ID for Wyoming is 56)\n",
    "4. Select only records where DAMAGE_PROPERTY + DAMAGE_CROPS >= min_damage\n",
    "5. Update the county FIPS for each record\n",
    "6. Ensure FIPS are 5 characters (for Plotly)\n",
    "7. Calculate the total damage in each county and build event list (some counties have more than one event)\n",
    "8. Prepare the dataframe for visualization using Plotly choropleth maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dd2835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_storm_data(year, files_dict):\n",
    "    url = NOAA_CSVFILES_URL + files_dict[f'{year}']\n",
    "    df_counties = pd.read_csv(url)\n",
    "    zone_county_df = get_zone_county_corr_df()\n",
    "\n",
    "    # 1. Drop rows that have STATE_FIPS = Nan or CZ_FIPS = Nan\n",
    "    df_counties.dropna(subset=['STATE_FIPS', 'CZ_FIPS'], inplace=True)\n",
    "\n",
    "    # 2. Convert DAMAGE_PROPERTY, DAMAGE_CROPS to numbers (replace Nan with 0)\n",
    "    def convertStrToNum(st) -> float:\n",
    "        '''\n",
    "        convert str to float\n",
    "        example: 3.00K --> 3000.0\n",
    "        '''\n",
    "        if type(st) is float:\n",
    "            return 0.0 if np.isnan(st) else float(st)\n",
    "        \n",
    "        st = st.strip()\n",
    "        if len(st) == 0:\n",
    "            return 0.0\n",
    "        elif st[-1] == 'k' or st[-1] == 'K':\n",
    "            return float(st[:-1])*1_000.0 if len(st) > 1 else 1_000\n",
    "        elif st[-1] == 'm' or st[-1] == 'M':\n",
    "            return float(st[:-1])*1_000_000.0 if len(st) > 1 else 1_000_000\n",
    "        elif st[-1] == 'b' or st[-1] == 'B':\n",
    "            return float(st[:-1])*1_000_000_000.0 if len(st) > 1 else 1_000_000_000.0\n",
    "        else:\n",
    "            return 0.0\n",
    "\n",
    "    df_counties['DAMAGE_PROPERTY'] = df_counties['DAMAGE_PROPERTY'].map(convertStrToNum)\n",
    "    df_counties['DAMAGE_CROPS'] = df_counties['DAMAGE_CROPS'].map(convertStrToNum)\n",
    "\n",
    "    # print(df_counties['STATE_FIPS'].unique()) # Puerto Rico is 99, American Samoa, US Virgin Islands\n",
    "\n",
    "    # 3. Select only counties in 50 US states and Washington DC (FIPS ID for Wyoming is 56)\n",
    "    # 4. DAMAGE_PROPERTY + DAMAGE_CROPS >= min_damage\n",
    "    df_counties = df_counties[(df_counties['STATE_FIPS'] <= 56) & # for now only 50 US states\n",
    "                          (df_counties['DAMAGE_PROPERTY'] + df_counties['DAMAGE_CROPS'] >= MIN_DAMAGE)] # either DAMAGE_PROPERTY or DAMAGE_CROPS > min_damage\n",
    "\n",
    "    print(f\"Number of counties with damage more or equal to than {MIN_DAMAGE} in {year}: {df_counties.shape[0]}\")\n",
    "\n",
    "    # 5. Update the county FIPS for each record\n",
    "    def update_fips(row):\n",
    "        if row['CZ_TYPE'] == 'C': # when CZ_TYPE is 'C', the record covers a single county\n",
    "            row['FIPS'] = [int(row['STATE_FIPS'])*1000 + row['CZ_FIPS']] # calculate FIPS\n",
    "            return row\n",
    "        \n",
    "        # CZ_TYPE is 'Z' (zone), get the county FIPS for each county that is in the zone\n",
    "        st_id = row['STATE_FIPS']\n",
    "        cz_id = row['CZ_FIPS']\n",
    "        filter_rec = zone_county_df[(zone_county_df['STATE_FIPS'] == st_id) &\n",
    "                                    (zone_county_df['ZONE'] == cz_id)]\n",
    "        # more than 1 county can cover a zone\n",
    "        lst_fips = [fips for fips in filter_rec['FIPS']]\n",
    "        num_counties = len(lst_fips)\n",
    "        if num_counties:\n",
    "            row['FIPS'] = lst_fips\n",
    "            row['DAMAGE_PROPERTY'] = row['DAMAGE_PROPERTY']/num_counties # split the damage across counties evenly!\n",
    "            row['DAMAGE_CROPS'] = row['DAMAGE_CROPS']/num_counties\n",
    "        else:\n",
    "            row['FIPS'] = np.NaN\n",
    "        return row\n",
    "\n",
    "    df_counties = df_counties.apply(update_fips, axis=1)\n",
    "    df_counties.dropna(subset=['FIPS'], inplace=True)\n",
    "    df_counties = df_counties[df_counties['DAMAGE_PROPERTY'] + df_counties['DAMAGE_CROPS'] >= MIN_DAMAGE]\n",
    "    df_counties = df_counties.explode(column=['FIPS'], ignore_index=True)\n",
    "\n",
    "    # 6. Ensure FIPS are 5 characters (for plotly)\n",
    "    df_counties['FIPS'] = df_counties['FIPS'].astype(str) # cast it as string in order to zfill\n",
    "    df_counties['FIPS'] = df_counties['FIPS'].apply(lambda x: x.zfill(5))\n",
    "\n",
    "    # 7. calculate the total damage in each county and build event list\n",
    "    county_dict = {} # key is FIPS, value is a list [name, dict {EVENT_TYPE: BEGIN_YEARMONTH%100/BEGIN_DAY}, total damage]\n",
    "    df_counties['TOTAL_DAMAGE'] = 0\n",
    "    \n",
    "    df_counties['BEGIN_TIME'] = df_counties['BEGIN_TIME'].astype(str)\n",
    "    df_counties['BEGIN_TIME'] = df_counties['BEGIN_TIME'].str.zfill(4)\n",
    "    df_counties['END_TIME'] = df_counties['END_TIME'].astype(str)\n",
    "    df_counties['END_TIME'] = df_counties['END_TIME'].str.zfill(4)\n",
    "\n",
    "    def calculate_total_damage(row):\n",
    "        categories = STORM_CATEGORIES.keys()\n",
    "        for k, v in STORM_CATEGORIES.items():\n",
    "            searchStr = '|'.join(v)\n",
    "            row[k] = re.search(searchStr, row['EVENT_TYPE']) != None\n",
    "\n",
    "        county_id = row['FIPS']\n",
    "        if county_id not in county_dict:\n",
    "            county_dict[county_id] = [row['CZ_NAME'] + ', ' + row['STATE'], {}, 0, {}, 0, {}, 0, {}, 0]\n",
    "            county_dict[county_id].extend([False]*len(categories))\n",
    "        \n",
    "        eventDateStr = str(row['BEGIN_YEARMONTH']%100) + '/' + str(row['BEGIN_DAY'])\n",
    "        if row['EVENT_TYPE'] not in county_dict[county_id][1]:\n",
    "            county_dict[county_id][1][row['EVENT_TYPE']] = set()\n",
    "            county_dict[county_id][1][row['EVENT_TYPE']].add(eventDateStr)\n",
    "        else:\n",
    "            county_dict[county_id][1][row['EVENT_TYPE']].add(eventDateStr)\n",
    "\n",
    "        county_dict[county_id][2] = county_dict[county_id][2] + row['DAMAGE_PROPERTY'] + row['DAMAGE_CROPS']\n",
    "        for i, c in enumerate(categories):\n",
    "            if row[c]:\n",
    "                county_dict[county_id][i+9] = row[c]\n",
    "                if row['EVENT_TYPE'] not in county_dict[county_id][2*i+3]:\n",
    "                    county_dict[county_id][2*i+3][row['EVENT_TYPE']] = set()\n",
    "                    county_dict[county_id][2*i+3][row['EVENT_TYPE']].add(eventDateStr)\n",
    "                else:\n",
    "                    county_dict[county_id][2*i+3][row['EVENT_TYPE']].add(eventDateStr)\n",
    "                county_dict[county_id][2*i+4] += row['DAMAGE_PROPERTY'] + row['DAMAGE_CROPS']\n",
    "\n",
    "        row['EVENT_DATE'] = pd.to_datetime(str(row['BEGIN_YEARMONTH']) + str(row['BEGIN_DAY']), format='%Y%m%d')\n",
    "        row['NAME'] = row['CZ_NAME'] + ', ' + row['STATE']\n",
    "        row['TOTAL_DAMAGE'] = row['DAMAGE_PROPERTY'] + row['DAMAGE_CROPS']\n",
    "        \n",
    "        begin_ts = pd.to_datetime(str(row['BEGIN_YEARMONTH'])[:4] + '-' + \\\n",
    "                                  str(row['BEGIN_YEARMONTH'])[4:] + '-' + \\\n",
    "                                  str(row['BEGIN_DAY']) + ':' + row['BEGIN_TIME'][:2] + ':' + row['BEGIN_TIME'][2:],\n",
    "                                  format = '%Y-%m-%d:%H:%M')\n",
    "        end_ts = pd.to_datetime(str(row['END_YEARMONTH'])[:4] + '-' + \\\n",
    "                                str(row['END_YEARMONTH'])[4:] + '-' + \\\n",
    "                                str(row['END_DAY']) + ':' + row['END_TIME'][:2] + ':' + row['END_TIME'][2:],\n",
    "                                format = '%Y-%m-%d:%H:%M')\n",
    "        row['DURATION'] = end_ts - begin_ts\n",
    "        row['TORNADO_STRENGTH'] = ''\n",
    "        if row['EVENT_TYPE'] == 'Tornado':\n",
    "            row['TORNADO_STRENGTH'] = row['TOR_F_SCALE'] + ',' + \\\n",
    "                                      str(row['TOR_LENGTH']) + ',' + str(row['TOR_WIDTH'])\n",
    "        \n",
    "        return row\n",
    "\n",
    "    df_counties = df_counties.apply(calculate_total_damage, axis=1)\n",
    "\n",
    "    lst_columns = TABLE_COLUMNS_NOAA.copy() # make sure to take a copy of the list of columns\n",
    "    lst_columns.extend(STORM_CATEGORIES.keys())\n",
    "    df_counties = df_counties[lst_columns] # completes df_counties\n",
    "\n",
    "    # prepare df_map for Plotly maps\n",
    "    lst_columns = ['NAME', 'ALL_EVENTS', 'TOTAL_DAMAGE', \n",
    "                           'EVENT_TYPE_0', 'TYPE_0_DAMAGE',\n",
    "                           'EVENT_TYPE_1', 'TYPE_1_DAMAGE',\n",
    "                           'EVENT_TYPE_2', 'TYPE_2_DAMAGE']\n",
    "    lst_columns.extend(STORM_CATEGORIES.keys())\n",
    "    \n",
    "    df_map = pd.DataFrame.from_dict(county_dict, orient='index', columns=lst_columns)\n",
    "    df_map.reset_index(inplace=True)\n",
    "    df_map.rename(columns={'index': 'FIPS'}, inplace=True)\n",
    "\n",
    "    def build_event_type(row):\n",
    "        event_desc = ''\n",
    "        for k, v in row.items():\n",
    "            if len(event_desc) != 0:\n",
    "                event_desc += '; '\n",
    "            event_desc += k + ': ' + ', '.join(v) # v is a set\n",
    "        return event_desc\n",
    "    df_map['ALL_EVENTS'] = df_map['ALL_EVENTS'].apply(build_event_type)\n",
    "    df_map['EVENT_TYPE_0'] = df_map['EVENT_TYPE_0'].apply(build_event_type)\n",
    "    df_map['EVENT_TYPE_1'] = df_map['EVENT_TYPE_1'].apply(build_event_type)\n",
    "    df_map['EVENT_TYPE_2'] = df_map['EVENT_TYPE_2'].apply(build_event_type)\n",
    "\n",
    "    return df_map, df_counties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bcb0c1",
   "metadata": {},
   "source": [
    "## Get the population in a county using census.gov api\n",
    "\n",
    "- American Community Survey 5-Year Data (2009-2019)\n",
    "\n",
    "Ref: https://www.census.gov/data/developers/data-sets/acs-5year.html\n",
    "\n",
    "Unlike the 1-year estimates, geographies do not have to meet a particular population threshold in order to be published.\n",
    "\n",
    "- Decennial Census (2010, 2000)\n",
    "\n",
    "Ref: https://www.census.gov/data/developers/data-sets/decennial-census.2000.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f1ad9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_from_census_api(url):\n",
    "    r = requests.get(url)\n",
    "    content = r.content # content is a Python string, parse the string to format the content into csv\n",
    "    content = re.sub(PATTERN, \"\", content.decode('utf-8'))\n",
    "\n",
    "    if len(r.content):\n",
    "        content = r.content\n",
    "        content = re.sub(PATTERN, \"\", content.decode('utf-8'))\n",
    "        df = pd.read_csv(io.StringIO(content), quotechar='\"')\n",
    "        return df\n",
    "\n",
    "    return None\n",
    "    \n",
    "def get_county_pop_census(year, county, state): # get population from decennial census in 2000, 2010\n",
    "    url = f'https://api.census.gov/data/2000/dec/sf1?get=NAME,P001001&for=county:{county}&in=state:{state}&key={CENSUS_API_KEY}'\n",
    "    df = get_results_from_census_api(url)\n",
    "    if df is not None:\n",
    "        url = f'https://api.census.gov/data/2010/dec/sf1?get=NAME,P001001&for=county:{county}&in=state:{state}&key={CENSUS_API_KEY}'\n",
    "        df_2010 = get_results_from_census_api(url)\n",
    "        if df_2010 is not None:\n",
    "            pop_2000 = df.iloc[0]['P001001']\n",
    "            pop_2010 = df_2010.iloc[0]['P001001']\n",
    "            #print(county, ', ', state, ', ', pop_2000, ' to ', pop_2010)\n",
    "            #print(pop_2000 + (year - 2000)*((pop_2010 - pop_2000)//10))\n",
    "            # interpolate the population for years between 2001 and 2008 from population from year 2000 and 2010\n",
    "            return pop_2000 + (year - 2000)*((pop_2010 - pop_2000)//10)\n",
    "    return None            \n",
    "        \n",
    "def get_county_pop(df, year):\n",
    "    if (year > 2019):\n",
    "        year = 2019 # last year for which we have data available\n",
    "    \n",
    "    def get_pop(row):\n",
    "        state = row.FIPS[:2]  # state FIPS is the first 2 characters of FIPS\n",
    "        county = row.FIPS[2:] # county FIPS is the remaining characters of the 5 character FIPS\n",
    "        # row['Features'] = ['Population']\n",
    "        if year < 2009: # Population estimates program is from 2009 - 2019 only\n",
    "            pop = get_county_pop_census(year, county, state)\n",
    "            if pop is not None:\n",
    "                row['DATA_COL'] = [str(pop)]\n",
    "            else:\n",
    "                print(f'No census pop record for {row[\"FIPS\"]}')\n",
    "                row['DATA_COL'] = ['NaN']\n",
    "            return row\n",
    "        \n",
    "        # go to the ACS url to get population estimate\n",
    "        url = f'https://api.census.gov/data/{year}/acs/acs5?get=NAME,B01001_001E&for=county:{county}&in=state:{state}&key={CENSUS_API_KEY}'\n",
    "        df = get_results_from_census_api(url)\n",
    "        if df is not None:\n",
    "            row['DATA_COL'] = [str(df.iloc[0]['B01001_001E'])]\n",
    "        else:\n",
    "            print(f'No census pop record for {row[\"FIPS\"]}')\n",
    "            row['DATA_COL'] = ['NaN']\n",
    "        return row\n",
    "\n",
    "    df = df.progress_apply(get_pop, axis = 1) # use tqdm to monitor progress\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fbb249",
   "metadata": {},
   "source": [
    "## Get the county business patterns data\n",
    "\n",
    "- County Business Patterns (1986-2019)\n",
    "\n",
    "Ref: https://www.census.gov/data/developers/data-sets/cbp-nonemp-zbp.html\n",
    "\n",
    "County Business Patterns provides annual statistics for businesses with paid employees within the U.S., Puerto Rico, and Island Areas at a detailed geography and industry level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0b0825",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_county_business_patterns(df, year):\n",
    "    if (year > 2019):\n",
    "        year = 2019 # last year for which we have data available\n",
    "    def get_cbp(row):\n",
    "        state = row.FIPS[:2]\n",
    "        county = row.FIPS[2:]\n",
    "        # row['Features'].extend(['County Business Patterns', '# establishments','Annual payroll($1000)','# employees'])\n",
    "        row['DATA_COL'].extend([' ']) # empty string to separate the heading: 'County Business Patterns'\n",
    "\n",
    "        if year >= 2015:\n",
    "            url = f'https://api.census.gov/data/{year}/cbp?get=NAME,ESTAB,PAYANN,EMP&for=county:{county}&in=state:{state}&key={CENSUS_API_KEY}'\n",
    "        else:\n",
    "            url = f'https://api.census.gov/data/{year}/cbp?get=ESTAB,PAYANN,EMP&for=county:{county}&in=state:{state}&key={CENSUS_API_KEY}'\n",
    "        r = requests.get(url)\n",
    "        if len(r.content):\n",
    "            content = r.content\n",
    "            content = re.sub(PATTERN, \"\", content.decode('utf-8'))\n",
    "            df = pd.read_csv(io.StringIO(content), quotechar='\"')            \n",
    "            row['DATA_COL'].extend([str(df.iloc[0]['ESTAB']), str(df.iloc[0]['PAYANN']), str(df.iloc[0]['EMP'])])\n",
    "        else:\n",
    "            print(f'No cbp record for {row[\"FIPS\"]}')\n",
    "            row['DATA_COL'].extend(['NaN']*3)\n",
    "        return row\n",
    "\n",
    "    df = df.progress_apply(get_cbp, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7ff641",
   "metadata": {},
   "source": [
    "## Get the non-employer statistics\n",
    "\n",
    "- Nonemployer Statistics (1997-2018)\n",
    "\n",
    "Ref: https://www.census.gov/data/developers/data-sets/cbp-nonemp-zbp.html\n",
    "\n",
    "Nonemployer Statistics provides annual statistics on U.S. businesses with no paid employees or payroll at a detailed geography and industry level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c979f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_county_non_emp_stats(df, year): # upto 2018\n",
    "    if (year > 2018):\n",
    "        year = 2018\n",
    "    def get_neb(row):\n",
    "        state = row.FIPS[:2]\n",
    "        county = row.FIPS[2:]\n",
    "        # row['Features'].extend(['Nonemployer Statistics', '# establishments','Revenue($1,000)'])\n",
    "        row['DATA_COL'].extend(' ') # empty string to separate the heading: 'Nonemployer Statistics'\n",
    "\n",
    "        if year >= 2015:\n",
    "            url = f'https://api.census.gov/data/{year}/nonemp?get=NAME,NESTAB,NRCPTOT&for=county:{county}&in=state:{state}&key={CENSUS_API_KEY}'\n",
    "        else:\n",
    "            url = f'https://api.census.gov/data/{year}/nonemp?get=NESTAB,NRCPTOT&for=county:{county}&in=state:{state}&key={CENSUS_API_KEY}'\n",
    "        r = requests.get(url)\n",
    "        if len(r.content):\n",
    "            content = r.content\n",
    "            content = re.sub(PATTERN, \"\", content.decode('utf-8'))\n",
    "            df = pd.read_csv(io.StringIO(content), quotechar='\"')\n",
    "            \n",
    "            row['DATA_COL'].extend([str(df.iloc[0]['NESTAB']), str(df.iloc[0]['NRCPTOT'])])\n",
    "        else:\n",
    "            print(f'No neb record for {row[\"FIPS\"]}')\n",
    "            row['DATA_COL'].extend(['NaN']*2)\n",
    "        return row\n",
    "\n",
    "    df = df.progress_apply(get_neb, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68b4eac",
   "metadata": {},
   "source": [
    "## Get the county economic data\n",
    "\n",
    "- Economic Census (2017, 2012, 2007, 2002)\n",
    "\n",
    "Ref: https://www.census.gov/data/developers/data-sets/economic-census.html\n",
    "\n",
    "The Economic Census is the official measure of the Nation’s businesses and economy. Conducted every five years, the survey serves as the statistical benchmark for current economic activity, such as, the Gross Domestic Product and the Producer Price Index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a53b9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_county_economy(df, year):\n",
    "    code_list = ''\n",
    "    census_api_key = f'&key={CENSUS_API_KEY}'\n",
    "    if year >= 2017:\n",
    "        year = 2017\n",
    "        base_url = f'https://api.census.gov/data/2017/ecnbasic?get=NAICS2017,ESTAB,EMP,RCPTOT&for=county:'#{county_fips}&in=state:{state_fips}'\n",
    "\n",
    "        for code in NAICS_CODE_DICT.keys():\n",
    "            code_list += ('&NAICS2017=' + code)\n",
    "    elif year >= 2012:\n",
    "        year = 2012\n",
    "        base_url = f'https://api.census.gov/data/2012/ewks?get=NAICS2012,ESTAB,EMP,RCPTOT,OPTAX&for=county:'#{county_fips}&in=state:{state_fips}'\n",
    "        for code in NAICS_CODE_DICT.keys():\n",
    "            code_list += ('&NAICS2012=' + code)\n",
    "    elif year >= 2007:\n",
    "        year = 2007\n",
    "        base_url = f'https://api.census.gov/data/2007/ewks?get=NAICS2007,ESTAB,EMP,RCPTOT,OPTAX&for=county:'#{county_fips}&in=state:{state_fips}'\n",
    "        for code in NAICS_CODE_DICT.keys():\n",
    "            code_list += ('&NAICS2007=' + code)\n",
    "    else:\n",
    "        year = 2002\n",
    "        base_url = f'https://api.census.gov/data/2002/ewks?get=NAICS2002,ESTAB,EMP,RCPTOT,OPTAX&for=county:'#{county_fips}&in=state:{state_fips}'\n",
    "        for code in NAICS_CODE_DICT.keys():\n",
    "            code_list += ('&NAICS2002=' + code)\n",
    "\n",
    "    def get_eco_data(row):\n",
    "        # print(row.FIPS)\n",
    "        state = row.FIPS[:2]\n",
    "        county = row.FIPS[2:]\n",
    "        # row['Features'].extend(['Economic Data', 'Rank #1 Industry', 'Value of business($1000)', '# establishments', '# employees'])\n",
    "        # row['Features'].extend(['Rank #2 Industry', 'Value of business($1000)', '# establishments', '# employees'])\n",
    "        # row['Features'].extend(['Rank #3 Industry', 'Value of business($1000)', '# establishments', '# employees'])\n",
    "        row['DATA_COL'].extend(' ') # Empty string to separate the heading: Economic Data\n",
    "            \n",
    "        url = base_url + f'{county}&in=state:{state}'\n",
    "        r = requests.get(url + code_list + census_api_key)\n",
    "        if len(r.content):\n",
    "            content = r.content\n",
    "\n",
    "            content = re.sub(PATTERN, \"\", content.decode('utf-8'))\n",
    "            code_col = f'NAICS{year}'\n",
    "            df_eco = pd.read_csv(io.StringIO(content), quotechar='\"')\n",
    "\n",
    "            lst_cols = [code_col,'ESTAB','EMP','RCPTOT']\n",
    "            df_eco = df_eco[lst_cols]\n",
    "\n",
    "            df_eco = df_eco.groupby([code_col], as_index=False).sum()\n",
    "            df_eco.sort_values('RCPTOT', ascending=False, inplace=True)\n",
    "\n",
    "            for i in range(3):\n",
    "                if df_eco.shape[0] > i:\n",
    "                    row['DATA_COL'].extend([NAICS_CODE_DICT[str(df_eco.iloc[i][code_col])], \n",
    "                                          str(df_eco.iloc[i]['RCPTOT']), \n",
    "                                          str(df_eco.iloc[i]['ESTAB']), \n",
    "                                          str(df_eco.iloc[i]['EMP'])])\n",
    "                else:\n",
    "                    row['DATA_COL'].extend(['NaN']*4)\n",
    "        else:\n",
    "            print(f'No econ record for {row[\"FIPS\"]}')\n",
    "            row['DATA_COL'].extend(['NaN']*12)\n",
    "        return row\n",
    "    \n",
    "    df = df.progress_apply(get_eco_data, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8e01db",
   "metadata": {},
   "source": [
    "## Main loop for getting the above data for every county in the storms data for a given year\n",
    "\n",
    "- Use tqdm to monitor progress\n",
    "- Save the data collected into the Amazon RDS database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c95630",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "files_dict = get_list_csvfiles(NOAA_CSVFILES_URL)\n",
    "\n",
    "# Create new `pandas` methods which use `tqdm` progress\n",
    "# (can use tqdm_gui, optional kwargs, etc.)\n",
    "tqdm.pandas()\n",
    "\n",
    "df_db = None\n",
    "for year in range(BEGIN_YEAR, END_YEAR):\n",
    "    df_map, df_counties = get_storm_data(year, files_dict)\n",
    "\n",
    "    df_counties = get_county_pop(df_counties, year)\n",
    "\n",
    "    df_counties = get_county_business_patterns(df_counties, year)\n",
    "\n",
    "    df_counties = get_county_non_emp_stats(df_counties, year)\n",
    "    \n",
    "    df_counties = get_county_economy(df_counties, year)\n",
    "    \n",
    "    df_counties['Year'] = year\n",
    "    if df_db is not None:\n",
    "        df_db = pd.concat([df_db, df_counties], axis=0, ignore_index=True)\n",
    "    else:\n",
    "        df_db = df_counties\n",
    "        \n",
    "df_db['DATA_COL'] = df_db['DATA_COL'].apply(lambda x : '|'.join(x))\n",
    "df_db['DATA_COL'] = df_db['DATA_COL'].astype(str)\n",
    "df_db\n",
    "\n",
    "# save the dataframe into Amazon RDS\n",
    "engine = create_engine(f'postgresql+psycopg2://{USER}:{PASSWORD}@{ENDPOINT}:{PORT}/{DATABASE}')\n",
    "with engine.begin() as connection:\n",
    "    df_db.to_sql('counties', con=connection, if_exists='append',index=False)\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650411ff",
   "metadata": {},
   "source": [
    "## Save the database into a backup csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23267b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(\n",
    "    host=ENDPOINT,\n",
    "    port=PORT,\n",
    "    user=USER,\n",
    "    password=PASSWORD,\n",
    "    database = DATABASE\n",
    ")\n",
    "with conn:\n",
    "    select = \"\"\"SELECT * from counties\"\"\"\n",
    "    df_counties = pd.read_sql(select, con=conn)\n",
    "    #delete = \"\"\"DELETE FROM counties WHERE \"Year\" = 2004\"\"\"\n",
    "    #conn.cursor().execute(delete)\n",
    "conn.close()\n",
    "\n",
    "# import copy\n",
    "# df_safecopy = pd.DataFrame(columns=df_counties.columns, data=copy.deepcopy(df_counties.values))\n",
    "# df_counties\n",
    "# df_counties.to_csv('../data/df_counties.csv') # Save a backup copy of the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556af6d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
